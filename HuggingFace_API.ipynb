{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Topeemma/Hugging_Face-API/blob/main/HuggingFace_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Getting Started with the Hugging Face Hub (Inference API)**\n",
        "\n",
        "* Heard of AI chat-based interfaces like **ChatGPT**, **Gemini**, **HuggingChat**?\n",
        "\n",
        "**What exactly is a Transformer-based language model?**\n",
        "\n",
        "These are large language models (LLMs) trained on massive datasets to understand and generate natural language. They are:\n",
        "\n",
        "* **Generative** â€” able to produce text and other content.\n",
        "* **Pre-trained** â€” trained in advance on large corpora.\n",
        "* **Transformer-based** â€” built on the transformer architecture that converts input to context-aware output.\n",
        "\n",
        "These models power many common NLP tasks: answering questions, summarizing content, translating languages, and generating human-like dialogue."
      ],
      "metadata": {
        "id": "cVuAgNVgtl_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Adding Your API Key to Colab**\n",
        "\n",
        "1. In Colab, click **ðŸ”‘ Secrets** in the left panel.\n",
        "2. Add a new secret with:\n",
        "\n",
        "   * **Name**: `HF_TOKEN`\n",
        "   * **Value**: *your API key*\n",
        "3. Grant notebook access to that secret.\n",
        "\n",
        "### **Loading the API Key in Your Code**\n",
        "\n",
        "First, retrieve the key securely:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F0aIQc1r74SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "PL6WGYuftpe8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_API_KEY=userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "hoFzueaf31vB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Then pass it to the SDK:**\n",
        "\n",
        "```python\n",
        "import os\n",
        "os.environ[\"HF_API_KEY\"] = HF_API_KEY\n",
        "\n",
        "from huggingface_hub import InferenceClient\n",
        "client = InferenceClient(token=os.environ[\"HF_API_KEY\"])\n",
        "```\n",
        "\n",
        "- <font color=\"red\">Warning</font>: Ensure that there are no whitespaces in your API key."
      ],
      "metadata": {
        "id": "xQlgleJw80CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install the Hugging Face Hub SDK**\n",
        "\n",
        "Now that your account and access token are ready, the next step is setting up your local environment. Weâ€™ll access models, datasets, and repos via the Hugging Face Hub Python library.\n",
        "\n",
        "You can install it using pip using the command below:\n"
      ],
      "metadata": {
        "id": "fZbXkk0wmHoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl80ZwM34HyZ",
        "outputId": "1b75bdb3-f997-425f-8c51-e8d0b63d1e80"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import packages**\n",
        "Import the necessary packages."
      ],
      "metadata": {
        "id": "AJeiBWD0nme6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import openai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "5E_hD-onmLCt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_API_KEY\"] = HF_API_KEY"
      ],
      "metadata": {
        "id": "ti0eRGvV8ntD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Instantiate a client**\n",
        "\n",
        "We will now create a client that can access various types of models globally. Please note that you should only provide an API key for authentication whenever you are initialising a client."
      ],
      "metadata": {
        "id": "eTaBwDF6nw9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "import os\n",
        "\n",
        "# Create client\n",
        "client = InferenceClient(token=os.environ[\"HF_API_KEY\"])\n",
        "\n",
        "prompt = \"Write a short story about Vegetables.\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "response = client.chat_completion(\n",
        "    messages=messages,\n",
        "    temperature=0.7,\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mGBg9416KAW",
        "outputId": "9e58fae2-f930-48f0-97d8-84a1d1c99578"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Title: The Vibrant Symphony of Verdant Harmony\n",
            "\n",
            "In a quaint, sun-kissed village nestled between rolling hills and a sparkling brook, there existed a peculiar community. This wasn't your ordinary village; it was inhabited by sentient vegetables, each with their own unique personalities, stories, and melodies.\n",
            "\n",
            "At the heart of the village was a vibrant patch of soil, known as Harmony Field. It was here that the vegetables grew, nourished by the love and songs of the villagers who tended to them. Each vegetable had a distinctive voice, and when they sang together, it created a symphony that resonated throughout the valley.\n",
            "\n",
            "One day, a small, timid lettuce named Leaflet arrived in the village. She was a stranger, having drifted downstream from a far-off land. The villagers, sensing her distress, welcomed her warmly, but Leaflet struggled to find her place. She had never heard the harmonious songs that filled the village, and her own voice was quiet and unassuming.\n",
            "\n",
            "One day, after a particularly trying day, Leaflet wandered into Harmony Field. As she gazed at the row of familiar faces, she felt a strange connection. She closed her eyes and began to hum a melody that had been echoing in her heart ever since she arrived.\n",
            "\n",
            "The villagers, who were practicing their harmonies, paused in surprise. The melody was unlike anything they had ever heard, yet it resonated with them on a deep, primal level. They invited Leaflet to join them, and as she began to sing, her melody intertwined with theirs, creating a symphony that was more beautiful and powerful than anything they had ever experienced.\n",
            "\n",
            "From that day forward, Leaflet became an integral part of the village. She learned to embrace her unique voice and the power it held. The villagers, in turn, learned to appreciate the beauty of diversity and the strength that came from unity.\n",
            "\n",
            "The vibrant symphony of verdant harmony continued to resonate throughout the valley, a testament to the power of acceptance, unity, and the beautiful, unique voices within each of us.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat_completion(\n",
        "    messages=messages,\n",
        "    model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFrrkzEN7Qp7",
        "outputId": "7cd29a39-00a2-429d-9619-adad1eafc9aa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a small, bustling town nestled between rolling hills and a sparkling river, there lived a curious little carrot named Carrot. Carrot was not like the other vegetables in the garden. He was always eager to explore and learn about the world beyond the fence. His friends, the other vegetables, often teased him for his adventurous spirit, but Carrot didn't mind. He believed that every vegetable had a story to tell, and he was determined to find them all.\n",
            "\n",
            "One sunny morning, as the dew still clung to the leaves, Carrot decided to venture out of the garden. He hopped over the fence and into the nearby meadow, where he met a friendly sunflower named Sunny. Sunny was tall and proud, with a warm smile that made Carrot feel at ease.\n",
            "\n",
            "\"Hello, Carrot! Where are you off to today?\" Sunny asked, her petals rustling gently in the breeze.\n",
            "\n",
            "\"I'm on a quest to find the stories of all the vegetables in the world,\" Carrot replied excitedly. \"I want to know what it's like to be a pepper, a potato, or even a tomato!\"\n",
            "\n",
            "Sunny chuckled. \"Well, you've come to the right place, my friend. I can tell you a bit about being a sunflower, and maybe we can find some other vegetables to talk to.\"\n",
            "\n",
            "Together, they wandered through the meadow, chatting about the different vegetables they encountered. They met a cucumber named Cucumber, who shared tales of his journey from the vine to the plate. They spoke with a lettuce named Lettuce, who had stories of her life in the salad bowl. Each vegetable had a unique story to tell, and Carrot listened intently, soaking up every detail.\n",
            "\n",
            "As the day wore on, the sun began to set, painting the sky in hues of orange and pink. Carrot and Sunny found a cozy spot under a tree, where they sat and shared stories until the stars twinkled in the night sky.\n",
            "\n",
            "\"Thank you, Sunny,\" Carrot said, feeling grateful for the day's adventures. \"I've learned so much about the vegetables in the world, and I can't wait to continue my journey.\"\n",
            "\n",
            "Sunny smiled, her face glowing in the moonlight. \"Remember, Carrot, every vegetable has a story, and every story is important. Keep exploring, and you'll find many more tales to tell.\"\n",
            "\n",
            "With a final wave, Carrot hopped back over the fence, ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat_completion(\n",
        "    messages=messages,\n",
        "    temperature=0.7,\n",
        "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZJEB6hz8M34",
        "outputId": "27640054-ec43-4ac6-c2e2-f59e69396d8d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**The Great Harvest Festival**\n",
            "\n",
            "In a lush green valley, surrounded by rolling hills and a sparkling river, the vegetables of the land gathered to celebrate the annual Harvest Festival. It was a time of great excitement and anticipation, as the vegetables had spent all year working tirelessly to grow and ripen in the warm sunshine.\n",
            "\n",
            "At the center of the festival was a magnificent display of brightly colored vegetables, each one showcasing its unique beauty and charm. There was Ruby, the radiant red tomato, whose vibrant skin glowed like a sunset. Next to her stood Emerald, the emerald-green broccoli, whose tightly packed florets seemed to shimmer in the sunlight. Nearby, a cluster of golden-yellow corn stalks swayed gently in the breeze, their delicate kernels glistening with dew.\n",
            "\n",
            "As the festival began, the vegetables took turns showcasing their skills and talents. Sammy the sweet potato danced a lively jig, his vibrant orange skin flashing with each step. Meanwhile, Lola the lettuce performed a delicate ballet, her crisp leaves rustling softly as she twirled across the stage.\n",
            "\n",
            "But the highlight of the festival was the Great Vegetable Cook-Off, where the vegetables competed to create the most delicious and mouth-watering dish. The competition was fierce, with each vegetable eager to showcase its unique flavor and texture. In the end, it was a close call between Benny the bell pepper and Carlos the carrot, but in the end, Benny's zesty salsa won over the judges' hearts.\n",
            "\n",
            "As the sun began to set, the festival came to a close, and the vegetables gathered to share in the bounty of the harvest. They feasted on a sumptuous feast of fresh vegetables, each one savoring the rich flavors and textures of their fellow vegetables.\n",
            "\n",
            "As the stars began to twinkle in the night sky, the vegetables reflected on the joy and camaraderie of the festival. They knew that they were more than just individual vegetables, working together to create a vibrant and thriving community. And as they drifted off to sleep, dreaming of next year's Harvest Festival, they knew that they would always stand together, united in their love of growth, harvest, and community.\n",
            "\n",
            "And so, the vegetables of the valley slumbered, their dreams infused with the promise of a new season, filled with growth, abundance, and the joy of the Harvest Festival.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What models can be used with the Python SDK?**\n",
        "\n",
        "Now you're ready to call models via the Hugging Face Inference API. Before generating responses, letâ€™s explore the models available for use with the SDK.\n",
        "\n",
        "For a more holistic view of available models, see the [Hugging Face Model Hub](https://huggingface.co/models).\n"
      ],
      "metadata": {
        "id": "CKptftZGoLhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize API\n",
        "api = HfApi()\n",
        "\n",
        "models = api.list_models(\n",
        "    task=\"text-generation\",\n",
        "    library=\"transformers\",\n",
        "    sort=\"downloads\",\n",
        "    direction=-1,\n",
        "    limit=50  # Get top 50\n",
        ")\n",
        "\n",
        "# Display as a list\n",
        "model_list = []\n",
        "for model in models:\n",
        "    model_list.append({\n",
        "        \"Model ID\": model.id,\n",
        "        \"Downloads\": model.downloads if hasattr(model, 'downloads') else 0,\n",
        "        \"Likes\": model.likes if hasattr(model, 'likes') else 0,\n",
        "        \"Tags\": \", \".join(model.tags[:3]) if model.tags else \"\"\n",
        "    })\n",
        "\n",
        "# Show as DataFrame\n",
        "df = pd.DataFrame(model_list)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cJL5oGdD84xN",
        "outputId": "29ad60e0-b7d1-43d9-922b-0ff3a5dca6bb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in 'list_models': task, library. Will not be supported from version '1.0'.\n",
            "\n",
            "Use `filter` instead.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Model ID  ...                                       Tags\n",
              "0                               openai-community/gpt2  ...                  transformers, pytorch, tf\n",
              "1                            Qwen/Qwen2.5-7B-Instruct  ...           transformers, safetensors, qwen2\n",
              "2                                     Qwen/Qwen3-0.6B  ...           transformers, safetensors, qwen3\n",
              "3                        Gensyn/Qwen2.5-0.5B-Instruct  ...           transformers, safetensors, qwen2\n",
              "4                    meta-llama/Llama-3.1-8B-Instruct  ...           transformers, safetensors, llama\n",
              "5                                  openai/gpt-oss-20b  ...         transformers, safetensors, gpt_oss\n",
              "6                       dphn/dolphin-2.9.1-yi-1.5-34b  ...           transformers, safetensors, llama\n",
              "7                                google/gemma-3-1b-it  ...     transformers, safetensors, gemma3_text\n",
              "8                           Qwen/Qwen3-Embedding-0.6B  ...  sentence-transformers, safetensors, qwen3\n",
              "9                  TinyLlama/TinyLlama-1.1B-Chat-v1.0  ...           transformers, safetensors, llama\n",
              "10                         Qwen/Qwen2.5-1.5B-Instruct  ...           transformers, safetensors, qwen2\n",
              "11                                  facebook/opt-125m  ...                  transformers, pytorch, tf\n",
              "12     trl-internal-testing/tiny-Qwen2ForCausalLM-2.5  ...           transformers, safetensors, qwen2\n",
              "13                   meta-llama/Llama-3.2-1B-Instruct  ...           transformers, safetensors, llama\n",
              "14                           Qwen/Qwen2.5-3B-Instruct  ...           transformers, safetensors, qwen2\n",
              "15                                openai/gpt-oss-120b  ...         transformers, safetensors, gpt_oss\n",
              "16                        Qwen/Qwen3-4B-Instruct-2507  ...           transformers, safetensors, qwen3\n",
              "17                             bigscience/bloomz-560m  ...         transformers, pytorch, tensorboard\n",
              "18  context-labs/meta-llama-Llama-3.2-3B-Instruct-...  ...           transformers, safetensors, llama\n",
              "19                 mistralai/Mistral-7B-Instruct-v0.2  ...         transformers, pytorch, safetensors\n",
              "20                                      Qwen/Qwen3-8B  ...           transformers, safetensors, qwen3\n",
              "21                              distilbert/distilgpt2  ...                  transformers, pytorch, tf\n",
              "22           deepseek-ai/DeepSeek-R1-Distill-Qwen-32B  ...           transformers, safetensors, qwen2\n",
              "23                   Qwen/Qwen3-Next-80B-A3B-Instruct  ...      transformers, safetensors, qwen3_next\n",
              "24                        inference-net/Schematron-3B  ...           transformers, safetensors, llama\n",
              "25                   meta-llama/Llama-3.2-3B-Instruct  ...           transformers, safetensors, llama\n",
              "26                          petals-team/StableBeluga2  ...           transformers, safetensors, llama\n",
              "27                            meta-llama/Llama-3.2-1B  ...           transformers, safetensors, llama\n",
              "28                                vikhyatk/moondream2  ...      transformers, safetensors, moondream1\n",
              "29                                     Qwen/Qwen3-32B  ...           transformers, safetensors, qwen3\n",
              "30                                    Qwen/Qwen2.5-7B  ...           transformers, safetensors, qwen2\n",
              "31                         meta-llama/Meta-Llama-3-8B  ...           transformers, safetensors, llama\n",
              "32                         Qwen/Qwen2.5-0.5B-Instruct  ...           transformers, safetensors, qwen2\n",
              "33                   microsoft/Phi-3-mini-4k-instruct  ...            transformers, safetensors, phi3\n",
              "34                             allenai/OLMo-2-0425-1B  ...           transformers, safetensors, olmo2\n",
              "35                           Qwen/Qwen3-Reranker-0.6B  ...           transformers, safetensors, qwen3\n",
              "36                        openai-community/gpt2-large  ...                  transformers, pytorch, tf\n",
              "37                                    Qwen/Qwen3-1.7B  ...           transformers, safetensors, qwen3\n",
              "38                   Qwen/Qwen3-30B-A3B-Instruct-2507  ...       transformers, safetensors, qwen3_moe\n",
              "39                                      Qwen/Qwen3-4B  ...           transformers, safetensors, qwen3\n",
              "40                                    google-t5/t5-3b  ...                  transformers, pytorch, tf\n",
              "41                             rednote-hilab/dots.ocr  ...     dots_ocr, safetensors, text-generation\n",
              "42                meta-llama/Meta-Llama-3-8B-Instruct  ...           transformers, safetensors, llama\n",
              "43                          Qwen/Qwen2.5-14B-Instruct  ...           transformers, safetensors, qwen2\n",
              "44          deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  ...           transformers, safetensors, qwen2\n",
              "45        unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit  ...           transformers, safetensors, llama\n",
              "46                      Qwen/Qwen2.5-32B-Instruct-AWQ  ...           transformers, safetensors, qwen2\n",
              "47                  meta-llama/Llama-3.3-70B-Instruct  ...           transformers, safetensors, llama\n",
              "48          kaitchup/Phi-3-mini-4k-instruct-gptq-4bit  ...            transformers, safetensors, phi3\n",
              "49                  meta-llama/Llama-3.1-70B-Instruct  ...           transformers, safetensors, llama\n",
              "\n",
              "[50 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df3cf2ed-8930-4b3b-939d-32da6af5f6b3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model ID</th>\n",
              "      <th>Downloads</th>\n",
              "      <th>Likes</th>\n",
              "      <th>Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>openai-community/gpt2</td>\n",
              "      <td>11286198</td>\n",
              "      <td>3003</td>\n",
              "      <td>transformers, pytorch, tf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Qwen/Qwen2.5-7B-Instruct</td>\n",
              "      <td>8037867</td>\n",
              "      <td>844</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Qwen/Qwen3-0.6B</td>\n",
              "      <td>7285383</td>\n",
              "      <td>745</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gensyn/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>6438470</td>\n",
              "      <td>26</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>meta-llama/Llama-3.1-8B-Instruct</td>\n",
              "      <td>5260341</td>\n",
              "      <td>4844</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>openai/gpt-oss-20b</td>\n",
              "      <td>4751782</td>\n",
              "      <td>3824</td>\n",
              "      <td>transformers, safetensors, gpt_oss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>dphn/dolphin-2.9.1-yi-1.5-34b</td>\n",
              "      <td>4724964</td>\n",
              "      <td>44</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>google/gemma-3-1b-it</td>\n",
              "      <td>4532452</td>\n",
              "      <td>677</td>\n",
              "      <td>transformers, safetensors, gemma3_text</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Qwen/Qwen3-Embedding-0.6B</td>\n",
              "      <td>4342260</td>\n",
              "      <td>694</td>\n",
              "      <td>sentence-transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
              "      <td>4329979</td>\n",
              "      <td>1437</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Qwen/Qwen2.5-1.5B-Instruct</td>\n",
              "      <td>4217697</td>\n",
              "      <td>533</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>facebook/opt-125m</td>\n",
              "      <td>4148297</td>\n",
              "      <td>222</td>\n",
              "      <td>transformers, pytorch, tf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>trl-internal-testing/tiny-Qwen2ForCausalLM-2.5</td>\n",
              "      <td>3973781</td>\n",
              "      <td>1</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
              "      <td>3918395</td>\n",
              "      <td>1137</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Qwen/Qwen2.5-3B-Instruct</td>\n",
              "      <td>3821561</td>\n",
              "      <td>322</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>openai/gpt-oss-120b</td>\n",
              "      <td>3747665</td>\n",
              "      <td>4077</td>\n",
              "      <td>transformers, safetensors, gpt_oss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
              "      <td>3703041</td>\n",
              "      <td>430</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>bigscience/bloomz-560m</td>\n",
              "      <td>3191538</td>\n",
              "      <td>129</td>\n",
              "      <td>transformers, pytorch, tensorboard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>context-labs/meta-llama-Llama-3.2-3B-Instruct-...</td>\n",
              "      <td>3141547</td>\n",
              "      <td>7</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
              "      <td>2777354</td>\n",
              "      <td>2996</td>\n",
              "      <td>transformers, pytorch, safetensors</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Qwen/Qwen3-8B</td>\n",
              "      <td>2503988</td>\n",
              "      <td>707</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>distilbert/distilgpt2</td>\n",
              "      <td>2338982</td>\n",
              "      <td>590</td>\n",
              "      <td>transformers, pytorch, tf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>deepseek-ai/DeepSeek-R1-Distill-Qwen-32B</td>\n",
              "      <td>2243915</td>\n",
              "      <td>1464</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Qwen/Qwen3-Next-80B-A3B-Instruct</td>\n",
              "      <td>2173107</td>\n",
              "      <td>843</td>\n",
              "      <td>transformers, safetensors, qwen3_next</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>inference-net/Schematron-3B</td>\n",
              "      <td>2069976</td>\n",
              "      <td>97</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
              "      <td>1917187</td>\n",
              "      <td>1786</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>petals-team/StableBeluga2</td>\n",
              "      <td>1825362</td>\n",
              "      <td>20</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>meta-llama/Llama-3.2-1B</td>\n",
              "      <td>1773268</td>\n",
              "      <td>2139</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>vikhyatk/moondream2</td>\n",
              "      <td>1749831</td>\n",
              "      <td>1322</td>\n",
              "      <td>transformers, safetensors, moondream1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Qwen/Qwen3-32B</td>\n",
              "      <td>1743121</td>\n",
              "      <td>560</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Qwen/Qwen2.5-7B</td>\n",
              "      <td>1729911</td>\n",
              "      <td>236</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
              "      <td>1691952</td>\n",
              "      <td>6355</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>1586508</td>\n",
              "      <td>383</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>microsoft/Phi-3-mini-4k-instruct</td>\n",
              "      <td>1575089</td>\n",
              "      <td>1316</td>\n",
              "      <td>transformers, safetensors, phi3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>allenai/OLMo-2-0425-1B</td>\n",
              "      <td>1539869</td>\n",
              "      <td>65</td>\n",
              "      <td>transformers, safetensors, olmo2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Qwen/Qwen3-Reranker-0.6B</td>\n",
              "      <td>1408952</td>\n",
              "      <td>247</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>openai-community/gpt2-large</td>\n",
              "      <td>1336341</td>\n",
              "      <td>335</td>\n",
              "      <td>transformers, pytorch, tf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Qwen/Qwen3-1.7B</td>\n",
              "      <td>1283805</td>\n",
              "      <td>302</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Qwen/Qwen3-30B-A3B-Instruct-2507</td>\n",
              "      <td>1278022</td>\n",
              "      <td>636</td>\n",
              "      <td>transformers, safetensors, qwen3_moe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Qwen/Qwen3-4B</td>\n",
              "      <td>1266307</td>\n",
              "      <td>428</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>google-t5/t5-3b</td>\n",
              "      <td>1221218</td>\n",
              "      <td>49</td>\n",
              "      <td>transformers, pytorch, tf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>rednote-hilab/dots.ocr</td>\n",
              "      <td>1175834</td>\n",
              "      <td>1101</td>\n",
              "      <td>dots_ocr, safetensors, text-generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
              "      <td>989368</td>\n",
              "      <td>4249</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Qwen/Qwen2.5-14B-Instruct</td>\n",
              "      <td>988102</td>\n",
              "      <td>279</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</td>\n",
              "      <td>965735</td>\n",
              "      <td>1367</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit</td>\n",
              "      <td>877599</td>\n",
              "      <td>85</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>Qwen/Qwen2.5-32B-Instruct-AWQ</td>\n",
              "      <td>829940</td>\n",
              "      <td>87</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>meta-llama/Llama-3.3-70B-Instruct</td>\n",
              "      <td>800311</td>\n",
              "      <td>2547</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>kaitchup/Phi-3-mini-4k-instruct-gptq-4bit</td>\n",
              "      <td>780925</td>\n",
              "      <td>2</td>\n",
              "      <td>transformers, safetensors, phi3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>meta-llama/Llama-3.1-70B-Instruct</td>\n",
              "      <td>759838</td>\n",
              "      <td>856</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df3cf2ed-8930-4b3b-939d-32da6af5f6b3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df3cf2ed-8930-4b3b-939d-32da6af5f6b3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df3cf2ed-8930-4b3b-939d-32da6af5f6b3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-684599cf-bfc1-4e54-bf39-2edec382de8f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-684599cf-bfc1-4e54-bf39-2edec382de8f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-684599cf-bfc1-4e54-bf39-2edec382de8f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_7e2afc7f-1007-4bfc-b642-a5a91828166f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7e2afc7f-1007-4bfc-b642-a5a91828166f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"Model ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"meta-llama/Llama-3.2-1B-Instruct\",\n          \"Qwen/Qwen3-4B\",\n          \"Qwen/Qwen2.5-7B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Downloads\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2132759,\n        \"min\": 759838,\n        \"max\": 11286198,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          3918395,\n          1266307,\n          1729911\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Likes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1431,\n        \"min\": 1,\n        \"max\": 6355,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          1137,\n          428,\n          236\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tags\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"transformers, safetensors, qwen3_next\",\n          \"transformers, safetensors, phi3\",\n          \"transformers, pytorch, tf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Making Your First API Call**\n",
        "\n",
        "In this section, youâ€™ll learn how to use the Hugging Face Inference API to send requests. For text tasks, youâ€™ll use endpoints designed for text generation or analysis. For image tasks, youâ€™ll use models specifically for image generation or classification(which we would cover later in this course).\n",
        "\n",
        "When interacting with chat-based models through Hugging Faceâ€™s `InferenceClient`, a common endpoint is the **chat completions** interface for instruction-following models.\n",
        "\n",
        "#### **Chat Completions API Overview**\n",
        "\n",
        "The chat-style API allows both single-turn and multi-turn interactions by processing a sequence of messages and generating coherent responses. It works well for both conversations and one-off queries.\n",
        "\n",
        "#### **Input Structure & Parameters**\n",
        "\n",
        "**A. Messages**\n",
        "You send a list of messages structured like:\n",
        "\n",
        "```python\n",
        "[\n",
        "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "  {\"role\": \"user\", \"content\": \"Tell me about Yoruba culture.\"}\n",
        "]\n",
        "```\n",
        "\n",
        "Each message contains:\n",
        "\n",
        "* **role**: Either `\"system\"`, `\"user\"`, or `\"assistant\"`\n",
        "* **content**: The actual message string\n",
        "\n",
        "**B. Max Tokens**\n",
        "Controls how much text the model should generate.\n",
        "\n",
        "```python\n",
        "response = client.chat_completion(\n",
        "    model=\"meta-llama/Llama-3-8B-Instruct\",\n",
        "    messages=messages,\n",
        "    max_tokens=150\n",
        ")\n",
        "```\n",
        "\n",
        "Tokens are subword units, not full words. For example, â€œunexpectedlyâ€ might tokenize to:\n",
        "`[\"un\", \"expect\", \"ed\", \"ly\"]`\n",
        "\n",
        "Each model has its own token limit. For example, Llama 3 (8B) supports up to ~8,192 tokens (input + output combined).\n",
        "\n",
        "**C. Temperature**\n",
        "Controls the randomness of the output. A lower temperature leads to more predictable, deterministic text; a higher temperature increases creativity and variety but may also increase the risk of irrelevant or nonsensical responses. The value ranges between 0 and 2:\n",
        "\n",
        "* `temperature = 0.0`: More deterministic\n",
        "* `temperature = 2.0`: More diverse and creative\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "response = client.chat_completion(\n",
        "    messages=messages,\n",
        "    temperature=0.7\n",
        ")\n",
        "```\n",
        "\n",
        "Higher values produce more varied results, lower values are better for accuracy and repetition control."
      ],
      "metadata": {
        "id": "JNBy4HsRoZTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define user Prompt\n",
        "prompt=\"Recommend to me a very interesting movie.\"\n",
        "# Creating a message as required by the API\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "completion = client.chat_completion(\n",
        "    messages = messages,\n",
        "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    max_tokens=300,\n",
        "    temperature=0.7,\n",
        ")\n",
        "print(completion)\n",
        "Markdown(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "_g26Y5-noZlY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "d527697a-3a6a-4f82-c6f9-b0333ae26c44"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='length', index=0, message=ChatCompletionOutputMessage(role='assistant', content='I\\'d like to recommend a thought-provoking and visually stunning movie that has received critical acclaim: \"Inception\" (2010) directed by Christopher Nolan.\\n\\n**Movie Synopsis:**\\n\\n\"Inception\" is a sci-fi action film that delves into the concept of shared dreaming. The movie follows Cobb (played by Leonardo DiCaprio), a skilled thief who specializes in entering people\\'s dreams and stealing their secrets. Cobb is tasked with performing a seemingly impossible task: planting an idea in someone\\'s mind instead of stealing one. This requires him to lead a team of experts into a dream within a dream within a dream, where the boundaries between reality and fantasy are blurred.\\n\\n**Why it\\'s interesting:**\\n\\n1. **Mind-bending plot:** The movie\\'s complex storyline will keep you guessing and questioning what\\'s real and what\\'s a dream.\\n2. **Innovative action sequences:** The film\\'s action scenes are expertly choreographed and visually stunning, with a blend of reality and fantasy.\\n3. **Philosophical themes:** \"Inception\" explores the nature of reality, identity, and the human mind, raising questions about the limits of human perception.\\n4. **Strong performances:** The all-star cast, including Leonardo DiCaprio, Joseph Gordon-Levitt, Ellen Page, and Tom Hardy, deliver memorable performances.\\n\\n**Awards and accolades:**\\n\\n\"Inception\" was nominated for eight Academy Awards and won four, including Best Cinematography and Best Sound Editing.', reasoning=None, tool_call_id=None, tool_calls=None), logprobs=None, content_filter_results={'hate': {'filtered': False}, 'self_harm': {'filtered': False}, 'sexual': {'filtered': False}, 'violence': {'filtered': False}, 'jailbreak': {'filtered': False, 'detected': False}, 'profanity': {'filtered': False, 'detected': False}})], created=1761834918, id='chatcmpl-ed0304a6de48492f8ff67e328e0b7ad5', model='meta-llama/llama-3.1-8b-instruct', system_fingerprint='', usage=ChatCompletionOutputUsage(completion_tokens=300, prompt_tokens=43, total_tokens=343, prompt_tokens_details=None, completion_tokens_details=None), object='chat.completion')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I'd like to recommend a thought-provoking and visually stunning movie that has received critical acclaim: \"Inception\" (2010) directed by Christopher Nolan.\n\n**Movie Synopsis:**\n\n\"Inception\" is a sci-fi action film that delves into the concept of shared dreaming. The movie follows Cobb (played by Leonardo DiCaprio), a skilled thief who specializes in entering people's dreams and stealing their secrets. Cobb is tasked with performing a seemingly impossible task: planting an idea in someone's mind instead of stealing one. This requires him to lead a team of experts into a dream within a dream within a dream, where the boundaries between reality and fantasy are blurred.\n\n**Why it's interesting:**\n\n1. **Mind-bending plot:** The movie's complex storyline will keep you guessing and questioning what's real and what's a dream.\n2. **Innovative action sequences:** The film's action scenes are expertly choreographed and visually stunning, with a blend of reality and fantasy.\n3. **Philosophical themes:** \"Inception\" explores the nature of reality, identity, and the human mind, raising questions about the limits of human perception.\n4. **Strong performances:** The all-star cast, including Leonardo DiCaprio, Joseph Gordon-Levitt, Ellen Page, and Tom Hardy, deliver memorable performances.\n\n**Awards and accolades:**\n\n\"Inception\" was nominated for eight Academy Awards and won four, including Best Cinematography and Best Sound Editing."
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will now learn how to have a multi-turn conversation with this LLM. To do this, we will add the assistant's response to the previous conversation and also include the new prompt in the same message format. After that, we will provide a list of dictionaries to the chat completion function.\n",
        "\n",
        "#### **Conversation Dynamics**\n",
        "Conversations can vary in length from a single message to a series of exchanges. Typically, these interactions might start with a system message to guide the assistant's behavior, followed by a sequence of alternating messages between the user and the assistant.\n",
        "\n",
        "#### **Roles Explained**\n",
        "- **System**: The system message sets the initial tone or guidelines for the assistantâ€™s behavior during the interaction. It can be used to imbue the assistant with a specific personality or to provide precise instructions on how it should conduct itself. While the system message is optional, its absence defaults the assistant's demeanor to that of a generally helpful nature, akin to starting with a message like \"You are a helpful assistant.\"\n",
        "  \n",
        "- **User**: Messages from the user generally consist of queries or comments that prompt responses from the assistant. These are the driving force of the conversation, guiding the topics and flow of the dialogue.\n",
        "\n",
        "- **Assistant**: This role involves messages generated in response to the user or system inputs. The assistant's messages can include responses based on previous interactions within the conversation. Alternatively, you can manually craft messages in this role to demonstrate preferred responses or to simulate typical interactions.\n",
        "\n",
        "By understanding and effectively utilizing these roles, you can create nuanced and dynamic dialogues tailored to specific interaction scenarios or conversational needs."
      ],
      "metadata": {
        "id": "C-GYbIFopSEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-turn conversation with system message\n",
        "response = client.chat_completion(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "fM4YKupWoZoa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "6a81bbd5-025d-407d-da40-8154fb385d2b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " The 2020 World Series was played at Globe Life Field in Arlington, Texas. The series was played without fans in attendance due to the COVID-19 pandemic."
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Gradio\n",
        "\n",
        "[Gradio](https://www.gradio.app/docs) is an openâ€‘source Python library that makes it easy to build a webâ€‘based interface around any Python function, model, or API. With just a few lines of code you can wrap your machineâ€‘learning model (or any processing function) into a usable UI and launch it locally or share it publicly. Gradio abstracts away the need for frontâ€‘end web development skills, enabling nonâ€‘technical users to interact with your model via browser input fields, sliders, image uploads, etc. Once your interface is running, you can also host it on platforms like Hugging Face Spaces so others can try it from anywhere.\n"
      ],
      "metadata": {
        "id": "QUqFTIPnTosN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet"
      ],
      "metadata": {
        "id": "dGN4xZB1CgsK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import gradio as gr\n",
        "import os\n",
        "\"meta-llama/Llama-3.1-8B-Instruct\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g5b_RRKTY2gr",
        "outputId": "c97a2fc2-785d-44bd-d6e0-41b1ab04aebb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'meta-llama/Llama-3.1-8B-Instruct'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory storage (keeps system + all past user/assistant messages)\n",
        "chat_history = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
        "]\n",
        "\n",
        "# Optional: keep only this many past messages (after the system message).\n",
        "# Set to None to keep everything (not recommended long-term).\n",
        "MAX_HISTORY_MESSAGES = 20\n"
      ],
      "metadata": {
        "id": "fco94-uenei_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_model(prompt):\n",
        "    try:\n",
        "        global chat_history\n",
        "\n",
        "        # Add the new user message to history\n",
        "        chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "\n",
        "\n",
        "        #  Optionally trim history to keep only the most recent messages\n",
        "        if MAX_HISTORY_MESSAGES is not None:\n",
        "            system_msg = chat_history[0]\n",
        "            tail = chat_history[1:]  # user+assistant messages\n",
        "            tail = tail[-MAX_HISTORY_MESSAGES:]\n",
        "            chat_history = [system_msg] + tail\n",
        "\n",
        "        #  Prepare messages to send to the model (a copy of the history)\n",
        "        messages = chat_history.copy()\n",
        "\n",
        "        #  Call the chat API with the full messages list so the model 'remembers'\n",
        "        response = client.chat_completion(\n",
        "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "            messages=messages,\n",
        "            max_tokens=500\n",
        "        )\n",
        "\n",
        "        #  Extract assistant text from the response (same as before)\n",
        "        assistant_text = response.choices[0].message.content\n",
        "\n",
        "        #  Append the assistant reply to history so future calls include it\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": assistant_text})\n",
        "\n",
        "        #  (optional) print debug info like you had before\n",
        "        print(assistant_text)\n",
        "        try:\n",
        "            print(response.usage.prompt_tokens)\n",
        "        except Exception:\n",
        "            pass  # some clients/models may not include usage info\n",
        "\n",
        "        return assistant_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "RCp6_YNkYvBI"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=chat_with_model,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask me anything...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Chat with AI Models\",\n",
        "    description=\"Ask the model any questions.\"\n",
        ")"
      ],
      "metadata": {
        "id": "BauzL2nCDt83"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "EtBGpcgiD0qj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "2f86b085-1be5-4d0a-fccc-49830289e0af"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f9be0c6fe9a620490a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f9be0c6fe9a620490a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Assignment: Add Memory to Your Chatbot**\n",
        "\n",
        "### Your Task\n",
        "Modify the chatbot function above, so it remembers previous messages in the conversation.\n",
        "\n",
        "### Current Problem\n",
        "Right now, your chatbot forgets everything after each response. If you say \"My name is Sarah\" and then ask \"What's my name?\", it won't remember.\n",
        "\n",
        "### What You Need to Do\n",
        "Make your chatbot remember all previous messages and responses, so it can refer back to earlier parts of the conversation.\n",
        "\n",
        "### Test Your Memory\n",
        "Your chatbot should be able to handle this conversation:\n",
        "```\n",
        "User: \"Hi, my name is Sarah and I love pizza.\"\n",
        "Bot: [responds]\n",
        "User: \"What's my name?\"\n",
        "Bot: [should say \"Sarah\"]\n",
        "User: \"What do I love?\"\n",
        "Bot: [should say \"pizza\"]\n",
        "```"
      ],
      "metadata": {
        "id": "Srt4_RvMTjbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hint**\n",
        "\n",
        "To give the chatbot a memory, youâ€™ll need to keep *all* of the past messages (user + bot) in a list of dictionaries behind the scenes, and then include that history every time you make a new API call. Hereâ€™s stepâ€‘byâ€‘step how you can do it:\n",
        "\n",
        "1. At the top of your script (outside the function) create a variable, e.g.\n",
        "\n",
        "   ```python\n",
        "   chat_history = []\n",
        "   ```\n",
        "\n",
        "   This will hold the sequence of all messages.\n",
        "\n",
        "2. Each time the user sends a prompt, add a dictionary representing the user message to `chat_history`, e.g.\n",
        "\n",
        "   ```python\n",
        "   chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "   ```\n",
        "\n",
        "3. Then when you call the model, pass *all* of the previous messages + the current user message as the `messages` list. For example:\n",
        "\n",
        "   ```python\n",
        "   messages = chat_history.copy()\n",
        "   messages.append({\"role\": \"assistant\", \"content\": ???})  # youâ€™ll do this after you get the response\n",
        "   ```\n",
        "\n",
        "4. After the model returns a response, take the assistantâ€™s reply content and append another dictionary into `chat_history`:\n",
        "\n",
        "   ```python\n",
        "   chat_history.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "   ```\n",
        "\n",
        "5. That way, when you next ask â€œWhatâ€™s my name?â€ or â€œWhat do I love?â€, the history contains the earlier statement (â€œMy name is Sarah and I love pizza.â€) and then the question follows. The model sees the entire chain and can answer accordingly.\n",
        "\n",
        "6. If you want, you can limit how many past messages you keep (for token/efficiency reasons) by slicing the list (e.g., `chat_history = chat_history[-10:]`).\n",
        "\n",
        "By following those steps, your chatbot will â€œrememberâ€ previous messages in the conversation."
      ],
      "metadata": {
        "id": "Z3lPefxCMOd-"
      }
    }
  ]
}